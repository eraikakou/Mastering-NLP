{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dccca4ba",
   "metadata": {},
   "source": [
    "You know that expression When you have a hammer, everything looks like a nail? Well, in machine learning, it seems like we really have discovered a magical hammer for which everything is, in fact, a nail, and they’re called Transformers. Transformers are models that can be designed to translate text, write poems and op eds, and even generate computer code. In fact, lots of the amazing research I write about on daleonai.com is built on Transformers, like AlphaFold 2, the model that predicts the structures of proteins from their genetic sequences, as well as powerful natural language processing (NLP) models like GPT-3, BERT, T5, Switch, Meena, and others. You might say they’re more than meets the… ugh, forget it.\n",
    "\n",
    "If you want to stay hip in machine learning and especially NLP, you have to know at least a bit about Transformers. So in this post, we’ll talk about what they are, how they work, and why they’ve been so impactful.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "## Background\n",
    "\n",
    "A Transformer is a type of neural network architecture. **To recap, neural nets are a very effective type of model for analyzing complex data types like images, videos, audio, and text.** **But there are different types of neural networks optimized for different types of data.** For example, for analyzing images, we’ll typically use convolutional neural networks or “CNNs.” Vaguely, they mimic the way the human brain processes visual information.\n",
    "\n",
    "<img width=\"1123\" alt=\"image\" src=\"https://user-images.githubusercontent.com/28102493/205899798-881814a3-fd1c-4f5b-926d-dc41f2f6880f.png\">\n",
    "\n",
    "And since around 2012, we’ve been quite successful at solving vision problems with CNNs, like identifying objects in photos, recognizing faces, and reading handwritten digits. But for a long time, nothing comparably good existed for language tasks (translation, text summarization, text generation, named entity recognition, etc). That was unfortunate, because language is the main way we humans communicate.\n",
    "\n",
    "**Before Transformers were introduced in 2017, the way we used deep learning to understand text was with a type of model called a Recurrent Neural Network or RNN** that looked something like this:\n",
    "\n",
    "\n",
    "<img width=\"1353\" alt=\"image\" src=\"https://user-images.githubusercontent.com/28102493/205900022-d42909b4-5d7f-4fd1-80d4-a142560d25eb.png\">\n",
    "\n",
    "\n",
    "Let’s say you wanted to translate a sentence from English to French. An RNN would take as input an English sentence, process the words one at a time, and then, sequentially, spit out their French counterparts. The key word here is “sequential.” In language, the order of words matters and you can’t just shuffle them around. The sentence:\n",
    "\n",
    "“Jane went looking for trouble.”\n",
    "\n",
    "means something very different from the sentence:\n",
    "\n",
    "“Trouble went looking for Jane”\n",
    "\n",
    "**So any model that’s going to understand language must capture word order, and recurrent neural networks did this by processing one word at a time, in a sequence.**\n",
    "\n",
    "But RNNs had issues:\n",
    "\n",
    "1. First, they struggled to handle large sequences of text, like long paragraphs or essays. By the time got to the end of a paragraph, they’d forget what happened at the beginning. An RNN-based translation model, for example, might have trouble remembering the gender of the subject of a long paragraph.\n",
    "\n",
    "1. Worse, RNNs were hard to train. They were notoriously susceptible to what’s called the vanishing/exploding gradient problem (sometimes you simply had to restart training and cross your fingers). \n",
    "\n",
    "1. Even more problematic, because they processed words sequentially, RNNs were hard to parallelize. This meant you couldn’t just speed up training by throwing more GPUs at the them, which meant, in turn, you couldn’t train them on all that much data.\n",
    "\n",
    "\n",
    "## Enter Transformers\n",
    "\n",
    "This is where Transformers changed everything. They were developed in 2017 by researchers at Google and the University of Toronto, initially designed to do translation. **But unlike recurrent neural networks, Transformers could be very efficiently parallelized.** And that meant, with the right hardware, you could train some really big models.\n",
    "\n",
    "GPT-3, the especially impressive text-generation model that writes almost as well as a human was trained on some 45 TB of text data, including almost all of the public web.\n",
    "\n",
    "So if you remember anything about Transformers, let it be this: combine a model that scales well with a huge dataset and the results will likely blow you away.\n",
    "\n",
    "\n",
    "The Transformer is an architecture that uses Attention to significantly improve the performance of deep learning NLP translation models. It was first introduced in the paper Attention is all you need and was quickly established as the leading architecture for most text data applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6277249",
   "metadata": {},
   "source": [
    "# What is a Transformer\n",
    "\n",
    "The Transformer architecture excels at handling text data which is inherently sequential. They take a text sequence as input and produce another text sequence as output. eg. to translate an input English sentence to Spanish.\n",
    "\n",
    "<img width=\"931\" alt=\"image\" src=\"https://user-images.githubusercontent.com/28102493/205924081-b8469f42-9e08-4233-9cd9-82316e3d8073.png\">\n",
    "\n",
    "At its core, it contains a stack of Encoder layers and Decoder layers. To avoid confusion we will refer to the individual layer as an Encoder or a Decoder and will use Encoder stack or Decoder stack for a group of Encoder layers.\n",
    "\n",
    "The Encoder stack and the Decoder stack each have their corresponding Embedding layers for their respective inputs. Finally, there is an Output layer to generate the final output. All the Encoders are identical to one another. Similarly, all the Decoders are identical.\n",
    "\n",
    "<img width=\"957\" alt=\"image\" src=\"https://user-images.githubusercontent.com/28102493/205925088-452639b6-3222-4827-8902-524953193e7b.png\">\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"906\" alt=\"image\" src=\"https://user-images.githubusercontent.com/28102493/205925309-2751c505-6a29-4c4d-bdd3-48c8611abcef.png\">\n",
    "\n",
    "\n",
    "1. The Encoder contains the all-important Self-attention layer that computes the relationship between different words in the sequence, as well as a Feed-forward layer.\n",
    "\n",
    "1. The Decoder contains the Self-attention layer and the Feed-forward layer, as well as a second Encoder-Decoder attention layer.\n",
    "\n",
    "1. **Each Encoder and Decoder has its own set of weights.**\n",
    "\n",
    "\n",
    "The Encoder is a reusable module that is the defining component of all Transformer architectures. In addition to the above two layers, it also has Residual skip connections around both layers along with two LayerNorm layers.\n",
    "\n",
    "<img width=\"937\" alt=\"image\" src=\"https://user-images.githubusercontent.com/28102493/205925971-eea11c4e-bf1a-4b81-96a8-50b2b92829e6.png\">\n",
    "\n",
    "\n",
    "**There are many variations of the Transformer architecture. Some Transformer architectures have no Decoder at all and rely only on the Encoder.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ccd1a",
   "metadata": {},
   "source": [
    "# How do Transformers Work?\n",
    "\n",
    "<img width=\"881\" alt=\"image\" src=\"https://user-images.githubusercontent.com/28102493/205901249-086573bb-37e5-47c1-9c85-4903c0904298.png\">\n",
    "\n",
    "Transformer diagram from the original paper\n",
    "\n",
    "While the diagram from the original paper is a little scary, the innovation behind Transformers boils down to three main concepts:\n",
    "\n",
    "1. **Positional Encodings**\n",
    "\n",
    "1. **Attention**\n",
    "\n",
    "1. **Self-Attention**\n",
    "\n",
    "\n",
    "## Positional Encodings\n",
    "\n",
    "Let’s start with the first one, positional encodings. Let’s say we’re trying to translate text from English to French. Remember that RNNs, the old way of doing translation, understood word order by processing words sequentially. But this is also what made them hard to parallelize.\n",
    "\n",
    "Transformers get around this barrier via an innovational called positional encodings. The idea is to take all of the words in your input sequence–an English sentence, in this case–and append each word with a number it’s order. So, you feed your network a sequence like:\n",
    "\n",
    "<img width=\"944\" alt=\"image\" src=\"https://user-images.githubusercontent.com/28102493/205903052-9fe4a55e-030e-4228-9cbf-b9d96c41f223.png\">\n",
    "\n",
    "**Conceptually, you can think of this as moving the burden of understanding word order from the structure of the neural network to the data itself.**\n",
    "\n",
    "**At first, before the Transformer has been trained on any data, it doesn’t know how to interpret these positional encodings. But as the model sees more and more examples of sentences and their encodings, it learns how to use them effectively.**\n",
    "\n",
    "I’ve done a bit of over-simplification here–the original authors used sine functions to come up with positional encodings, not the simple integers 1, 2, 3, 4–but the point is the same. **Store word order as data, not structure, and your neural network becomes easier to train.**\n",
    "\n",
    "\n",
    "## Attention\n",
    "\n",
    "Attention is a neural network structure that you’ll hear about all over the place in machine learning these days. In fact, the title of the 2017 paper that introduced Transformers wasn’t called, We Present You the Transformer. Instead it was called **Attention is All You Need.**\n",
    "\n",
    "Attention was introduced in the context of translation two years earlier, in **2015**. To understand it, take this example sentence from the original paper:\n",
    "\n",
    "The agreement on the European Economic Area was signed in August 1992.\n",
    "\n",
    "Now imagine trying to translate that sentence into its French equivalent:\n",
    "\n",
    "L’accord sur la zone économique européenne a été signé en août 1992.\n",
    "\n",
    "One bad way to try to translate that sentence would be to go through each word in the English sentence and try to spit out its French equivalent, one word at a time. That wouldn’t work well for several reasons, but for one, some words in the French translation are flipped: it’s “European Economic Area” in English, but “la zone économique européenne” in French. Also, French is a language with gendered words. The adjectives “économique” and “européenne” must be in feminine form to match the feminine object “la zone.”\n",
    "\n",
    "Attention is a mechanism that allows a text model to “look at” every single word in the original sentence when making a decision about how to translate words in the output sentence. Here’s a nice visualization from that original attention paper:\n",
    "\n",
    "<img width=\"949\" alt=\"image\" src=\"https://user-images.githubusercontent.com/28102493/205903675-d9c45f74-a1b9-4c1f-8bad-2402cc4a3fab.png\">\n",
    "\n",
    "It’s a sort of heat map that shows where the model is “attending” when it outputs each word in the French sentence. As you might expect, when the model outputs the word “européenne,” it’s attending heavily to both the input words “European” and “Economic.”\n",
    "\n",
    "And how does the model know which words it should be “attending” to at each time step? It’s something that’s learned from training data. By seeing thousands of examples of French and English sentences, the model learns what types of words are interdependent. **It learns how to respect gender, plurality, and other rules of grammar.**\n",
    "\n",
    "The attention mechanism has been an extremely useful tool for natural language processing since its discovery in 2015, but in its original form, it was used alongside recurrent neural networks. **So, the innovation of the 2017 Transformers paper was, in part, to ditch RNNs entirely. That’s why the 2017 paper was called “Attention is all you need.”**\n",
    "\n",
    "\n",
    "## Self-Attention\n",
    "\n",
    "The last (and maybe most impactful) piece of the Transformer is a twist on attention called “self-attention.”\n",
    "\n",
    "The type of “vanilla” attention we just talked about helped align words across English and French sentences, which is important for translation. But what if you’re not trying to translate words but instead build a model that understands underlying meaning and patterns in language–a type of model that could be used to do any number of language tasks?\n",
    "\n",
    "**In general, what makes neural networks powerful and exciting and cool is that they often automatically build up meaningful internal representations of the data they’re trained on.** When you inspect the layers of a vision neural network, for example, you’ll find sets of neurons that “recognize” edges, shapes, and even high-level structures like eyes and mouths. *A model trained on text data might automatically learn parts of speech, rules of grammar, and whether words are synonymous.*\n",
    "\n",
    "**The better the internal representation of language a neural network learns, the better it will be at any language task.** And it turns out that attention can be a very effective way of doing just this, if it’s turned on the input text itself.\n",
    "\n",
    "For example, take these two sentence:\n",
    "\n",
    "“Server, can I have the check?”\n",
    "\n",
    "“Looks like I just crashed the server.”\n",
    "\n",
    "The word server here means two very different things, which we humans can easily disambiguate by looking at surrounding words. Self-attention allows a neural network to understand a word in the context of the words around it.\n",
    "\n",
    "So when a model processes the word “server” in the first sentence, it might be “attending” to the word “check,” which helps disambiguate a human server from a metal one.\n",
    "\n",
    "In the second sentence, the model might attend to the word “crashed” to determine this “server” refers to a machine.\n",
    "\n",
    "Self-attention help neural networks disambiguate words, do part-of-speech tagging, entity resolution, learn semantic roles and a lot more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a9d5f",
   "metadata": {},
   "source": [
    "# Transformer-based Models\n",
    "\n",
    "One of the most popular Transformer-based models is called **BERT**, short for “Bidirectional Encoder Representations from Transformers.” It was introduced by researchers at Google in 2018, and soon made its way into almost every NLP project–including Google Search.\n",
    "\n",
    "**BERT refers not just a model architecture but to a trained model itself**, which you can download and use for free. It was trained by Google researchers on a massive text corpus and has become something of a general-purpose pocket knife for NLP. It can be extended solve a bunch of different tasks, like:\n",
    "\n",
    "- text summarization\n",
    "\n",
    "- question answering\n",
    "\n",
    "- classification\n",
    "\n",
    "- named entity resolution\n",
    "\n",
    "- text similarity\n",
    "\n",
    "- offensive message/profanity detection\n",
    "\n",
    "- understanding user queries\n",
    "\n",
    "- a whole lot more\n",
    "\n",
    "BERT proved that you could build very good language models trained on unlabeled data, like text scraped from Wikipedia and Reddit, and that these **large “base” models could then be adapted with domain-specific data to lots of different use cases.**\n",
    "\n",
    "\n",
    "More recently, the model **GPT-3**, created by **OpenAI**, has been blowing people’s minds with its ability to generate realistic text. **Meena**, introduced by **Google Research** last year, is a Transformer-based chatbot (akhem, “conversational agent”) that can have compelling conversations about almost any topic (this author once spent twenty minutes arguing with Meena about what it means to be human).\n",
    "\n",
    "Transformers have also been making waves outside of NLP, by **composing music**, **generating images from text descriptions**, and **predicting protein structure**.\n",
    "\n",
    "\n",
    "\n",
    "## Models List\n",
    "\n",
    "1. [ ] BERT, Google Research\n",
    "1. [ ] Roberta\n",
    "1. [ ] GPT-2\n",
    "1. [ ] T5\n",
    "1. [ ] GPT-3, OpenAI\n",
    "1. [ ] Meena, Google Research\n",
    "\n",
    "\n",
    "\n",
    "# How Can I Use Transformers?\n",
    "\n",
    "Now that you’re sold on the power of Transformers, you might want to know how you can start using them in your own app. No problemo.\n",
    "\n",
    "You can download common Transformer-based models like BERT from TensorFlow Hub. \n",
    "\n",
    "But if you want to be really trendy and you write Python, I’d highly recommend the popular “Transformers” library maintained by the company HuggingFace. The platform allow you to train and use most of today’s popular NLP models, like BERT, Roberta, T5, GPT-2, in a very developer-friendly way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ecd956",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Transformers, Explained: Understand the Model Behind GPT-3, BERT, and T5](https://daleonai.com/transformers-explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9efcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
