{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36aea8eb",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "## Definition\n",
    "\n",
    "- In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract “topics” that occur in a collection of documents. \n",
    "\n",
    "- Topic Modelling is a technique to extract hidden topics from large volumes of text. The technique we will be introducing is categorized as an unsupervised machine learning algorithm. \n",
    "\n",
    "Basically, we’re looking for what collections of words, or topics, are most relevant to discussing the content of the corpus. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09806863",
   "metadata": {},
   "source": [
    "# Python Libraries for Topic Modeling \n",
    "\n",
    "- **Gensim:** is one of the pre-eminent libraries for topic modeling \n",
    "\n",
    "- **spaCy:** is a powerful natural language processing library that has won a lot of admirers in the last few years. spaCy has several different models to choose from. We can use the large, general purpose web module, but use what makes sense to you. This will need to be downloaded through the command line. Check out the model page [here](https://spacy.io/models) for more info.\n",
    "\n",
    "- If you haven’t used **tqdm** before, it’s a module that allows you to create progress bars to track how long your code is taking to process.\n",
    "\n",
    "- **pprint** is to make our topics formatted a little nicer when we take a look.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c589d",
   "metadata": {},
   "source": [
    "# Topic Modeling Algorithms\n",
    "\n",
    "\n",
    "## Latent Dirichlet Allocation (LDA) \n",
    "\n",
    "LDA was first developed by Blei et al. in 2003. LDA is a generative probabilistic model similar to Naive Bayes. It represents topics as word probabilities and allows for uncovering latent or hidden topics as it clusters the words based on their co-occurrence in a respective document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c414c6",
   "metadata": {},
   "source": [
    "# Topic Modeling Pipeline\n",
    "\n",
    "\n",
    "1. We will start the process by collecting the documents (Step 1); \n",
    "\n",
    "2. afterwards, we will do some data cleaning and break down all the documents into tokens (Step 2). \n",
    "\n",
    "3. From the tokens, we can build a dictionary that gives each token a unique ID number, which can then be used to create a corpus or Bag of Words representing the frequency of the tokens (Step 3). \n",
    "\n",
    "4. we use our dictionary and corpus to build a range of topics and try to find the optimal number of topics (Step 4).\n",
    "\n",
    "5. The last step is to find the distribution of topics in each document (Step 5).\n",
    "\n",
    "<img width=\"929\" alt=\"image\" src=\"https://user-images.githubusercontent.com/28102493/205102250-fe272787-8a49-48e3-b3a7-b659d613005c.png\">\n",
    "\n",
    "\n",
    "## Step 1: Text Preprocessing\n",
    "\n",
    "The default spaCy pipeline is laid out like this:\n",
    "\n",
    "- **Tokenizer:** Breaks the full text into individual tokens.\n",
    "- **Tagger:** Tags each token with the part of speech.\n",
    "- **Parser:** Parses into noun chunks, amongst other things.\n",
    "- **Named Entity Recognizer (NER):** Labels named entities, like U.S.A.\n",
    "\n",
    "Of course, we don’t really need all of these elements every time. spaCy also allows you to build a custom pipeline using your own functions, in addition to what they have out of the box, and that’s where we will be getting the real value. spaCy has a robust stop words list and lemmatizer built in, but we’ll need to add that functionality into the pipeline.\n",
    "\n",
    "\n",
    "\n",
    "- **Stop words removal:** they are basically common words that don’t really add a lot of predictive value to your model. If you don’t remove the word “the” from your corpus for example, it will likely show up in every topic you generate, given how often “the” is used in the English language.\n",
    "\n",
    "- **Lemmatization:** We’ll also need to lemmatize the texts, which is simply reducing each word to it’s root. So for example, “going” and “goes” both are reduced to “go”. “Better” would be reduced to “good”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e36ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
